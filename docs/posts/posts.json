[
  {
    "path": "posts/2021-03-18-houseprice/",
    "title": "PREDICTION: HOUSE PRICE IN LONDON USING RANDOM FOREST",
    "description": "Predicting House Price in London using \"tidymodels\" in R-Studio",
    "author": [
      {
        "name": "Zahier Nasrudin",
        "url": {}
      }
    ],
    "date": "2021-03-18",
    "categories": [],
    "contents": "\r\nData Loading\r\nIn this post, we will be doing a prediction using Random Forest from Tidymodels in R. This data set can be obtained from https://www.kaggle.com/arnavkulkarni/housing-prices-in-london.\r\n\r\n\r\nlondon <- read_csv(here(\"_posts/2021-03-18-houseprice/london.csv\")) %>%\r\n  clean_names()\r\n\r\n\r\n\r\nDescription\r\nThe dataset contains 3480 observations and the features:\r\nProperty name\r\nPrice (£)\r\nHouse type\r\nArea (Square Feet)\r\nNumber of bedrooms\r\nNumber of bathrooms\r\nNumber of receptions\r\nLocation\r\nCity/Country\r\nPostal Code\r\n\r\n\r\nlondon %>% \r\n  glimpse()\r\n\r\n\r\nRows: 3,480\r\nColumns: 11\r\n$ x1               <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1~\r\n$ property_name    <chr> \"Queens Road\", \"Seward Street\", \"Hotham Roa~\r\n$ price            <dbl> 1675000, 650000, 735000, 1765000, 675000, 4~\r\n$ house_type       <chr> \"House\", \"Flat / Apartment\", \"Flat / Apartm~\r\n$ area_in_sq_ft    <dbl> 2716, 814, 761, 1986, 700, 403, 1548, 560, ~\r\n$ no_of_bedrooms   <dbl> 5, 2, 2, 4, 2, 1, 4, 1, 3, 2, 6, 6, 5, 2, 2~\r\n$ no_of_bathrooms  <dbl> 5, 2, 2, 4, 2, 1, 4, 1, 3, 2, 6, 6, 5, 2, 2~\r\n$ no_of_receptions <dbl> 5, 2, 2, 4, 2, 1, 4, 1, 3, 2, 6, 6, 5, 2, 2~\r\n$ location         <chr> \"Wimbledon\", \"Clerkenwell\", \"Putney\", \"Putn~\r\n$ city_county      <chr> \"London\", \"London\", \"London\", \"London\", \"Lo~\r\n$ postal_code      <chr> \"SW19 8NY\", \"EC1V 3PA\", \"SW15 1QL\", \"SW15 1~\r\n\r\nExploring Data\r\nProperty name\r\nTop 20 Property Name (Frequency)\r\n\r\n\r\nlondon %>%\r\n  count(property_name, sort = TRUE) %>%\r\n  head(20) %>%\r\n  mutate(property_name = fct_reorder(property_name, n)) %>%\r\n  ggplot(aes(x = property_name, y = n)) +\r\n  geom_col(fill = \"light blue\") +\r\n  coord_flip()+\r\n  labs(title = \"Property Name\",\r\n       subtitle = \"Top 20 (Frequency)\",\r\n       x = \"Property Name\",\r\n       y = \"Total\")\r\n\r\n\r\n\r\n\r\nHouse Type\r\n\r\n\r\nlondon <- london %>%\r\n  mutate(house_type = fct_lump(house_type, n = 3))\r\n\r\nlondon %>%\r\n  count(house_type, sort = TRUE) %>%\r\n  mutate(house_type = fct_reorder(house_type, n)) %>%\r\n  ggplot(aes(x = house_type, y = n)) +\r\n  geom_text(aes(label= n), \r\n                vjust=0.5, hjust=0, size = 3)+\r\n  geom_col(fill = \"light blue\") +\r\n  coord_flip(clip = \"off\")+\r\n  labs(title = \"House Type\",\r\n       x = \"House Type\",\r\n       y = \"Total\")\r\n\r\n\r\n\r\n\r\nHouse Price\r\nThe graph is skewed to the right, which means there are more lower price houses as compared to the higher price houses.\r\n\r\n\r\nlondon %>%\r\n  ggplot(aes(x = price)) +\r\n  geom_histogram(fill = \"light blue\") +\r\n  scale_y_continuous(labels = scales::comma_format()) +\r\n  scale_x_continuous(labels = scales::comma_format()) +\r\n  labs(title = \"House Price (Histogram)\",\r\n       x = \"House Price\",\r\n       y = \"Total\")\r\n\r\n\r\n\r\n\r\nHouse Price (Log 10)\r\n\r\n\r\nlondon %>%\r\n  ggplot(aes(x = price)) +\r\n  geom_histogram(fill = \"light blue\") +\r\n  scale_y_continuous(labels = scales::comma_format()) +\r\n  scale_x_log10(labels = scales::comma_format()) +\r\n  labs(title = \"House Price (Histogram)\",\r\n       subtitle = \"Log 10\",\r\n       x = \"House Price\",\r\n       y = \"Total\")\r\n\r\n\r\n\r\n\r\nPrice vs Area (Sq Ft)\r\nNotice that the most expensive house is only slightly higher than 5,000 sq ft in area size. We could also see that most of the houses are at the lower side of the price and the area size.\r\n\r\n\r\nlondon %>%\r\n  ggplot(aes(x = price, y = area_in_sq_ft, colour = house_type)) +\r\n  geom_point() +\r\n  scale_y_continuous(labels = scales::comma_format()) +\r\n  scale_x_continuous(labels = scales::comma_format()) +\r\n  labs(title = \"Price vs Area (Sq Ft)\",\r\n       x = \"House Price\",\r\n       y = \"Area (Sq ft)\",\r\n       colour = \"House Type\")\r\n\r\n\r\n\r\n\r\nArea Size and Number of bathrooms\r\nIt appears that the total number of bathrooms and the total number of bedrooms are the same. If it does, that does not make sense at all.\r\n\r\n\r\nlondon %>%\r\n  ggplot(aes(x = no_of_bedrooms, y = no_of_bathrooms, colour = house_type)) +\r\n  geom_point() +\r\n  scale_y_continuous(labels = scales::comma_format()) +\r\n  scale_x_continuous(labels = scales::comma_format()) +\r\n  labs(title = \"Bedrooms and Bathrooms\",\r\n       x = \"No of Bedrooms\",\r\n       y = \"No of bathrooms\",\r\n       colour = \"House Type\")\r\n\r\n\r\n\r\n\r\nLet’s check, just to be sure. We will check for the total number of receptions too\r\n\r\n\r\nlondon %>%\r\n  mutate(check_room = if_else(no_of_bedrooms == no_of_bathrooms,\r\n                              \"SAME\",\r\n                              \"DIFFERENT\"),\r\n         check_reception = if_else(no_of_bedrooms == no_of_receptions,\r\n                              \"SAME\",\r\n                              \"DIFFERENT\")) %>%\r\n\r\n  select(check_room, check_reception) %>%\r\n  unique()\r\n\r\n\r\n# A tibble: 1 x 2\r\n  check_room check_reception\r\n  <chr>      <chr>          \r\n1 SAME       SAME           \r\n\r\nYes, there is no difference between the total number of bathrooms and the total number of bedrooms / receptions. We will just use the number of bedrooms then.\r\nLocation\r\nLocation (Frequency). Too many missing values in the Location column. We will not include this column then for prediction later.\r\n\r\n\r\nlondon %>%\r\n  count(location, sort = TRUE) %>%\r\n  head(10) %>%\r\n  kableExtra::kable()\r\n\r\n\r\n\r\nlocation\r\n\r\n\r\nn\r\n\r\n\r\nNA\r\n\r\n\r\n962\r\n\r\n\r\nPutney\r\n\r\n\r\n126\r\n\r\n\r\nBattersea\r\n\r\n\r\n94\r\n\r\n\r\nEsher\r\n\r\n\r\n88\r\n\r\n\r\nWandsworth\r\n\r\n\r\n87\r\n\r\n\r\nWimbledon\r\n\r\n\r\n82\r\n\r\n\r\nBarnes\r\n\r\n\r\n81\r\n\r\n\r\nFulham\r\n\r\n\r\n74\r\n\r\n\r\nChiswick\r\n\r\n\r\n67\r\n\r\n\r\nRichmond\r\n\r\n\r\n63\r\n\r\n\r\nCity\r\n\r\n\r\nlondon <- london %>%\r\n    mutate(city_county = fct_lump(city_county, n = 3))\r\n\r\nlondon %>%\r\n  count(city_county) %>%\r\n  mutate(city_county = fct_reorder(city_county, -n)) %>%\r\n  ggplot(aes(x = city_county, y = n)) +\r\n  geom_col(fill = \"light blue\")\r\n\r\n\r\n\r\n\r\nLogarithm of House Price\r\nWe will use the logarithm of the house price for prediction. Features that we will use:\r\nHouse Type\r\nArea\r\nNumber of bedrooms\r\nCity country\r\nLogarithm of the house price (Outcome)\r\n\r\n\r\nlondon <- london %>%\r\n  mutate(price_log = log10(price))\r\n\r\nlondon_predict <- london %>%\r\n  select(house_type, area_in_sq_ft, no_of_bedrooms, city_county, price_log)\r\n\r\n\r\n\r\nTune Hyper-parameters\r\nSplit the data set into training and testing sets\r\nWe will then try to tune:\r\nmtry\r\ntrees\r\n\r\n\r\n\r\nlibrary(tidymodels)\r\n\r\nset.seed(1000)\r\n\r\n\r\nlondon_split <- initial_split(london_predict)\r\nlondon_train <- training(london_split)\r\nlondon_test <- testing(london_split)\r\n\r\nlondon_split\r\n\r\n\r\n<Analysis/Assess/Total>\r\n<2610/870/3480>\r\n\r\nRecipe\r\nDefine a recipe\r\nWe will be adding one extra process, where we will change city_contry into dummy variables\r\n\r\n\r\nprice_recipe <- london_train %>%\r\n  recipe(price_log ~.) %>%\r\n  step_dummy(city_county)\r\n\r\n\r\n\r\nWorkflow\r\nDefine a workflow\r\n\r\n\r\nprice_workflow <- workflow() %>%\r\n  add_recipe(price_recipe)\r\n\r\n\r\n\r\nWe will then define model specifications\r\n\r\n\r\nrandom_forest_model <- rand_forest(\r\n  mode = \"regression\",\r\n  mtry = tune(),\r\n  trees = tune()\r\n) %>%\r\n  set_engine(\"ranger\")\r\n\r\nrandom_forest_model\r\n\r\n\r\nRandom Forest Model Specification (regression)\r\n\r\nMain Arguments:\r\n  mtry = tune()\r\n  trees = tune()\r\n\r\nComputational engine: ranger \r\n\r\n\r\n\r\nrandom_forest_workflow <- price_workflow %>%\r\n  add_model(random_forest_model)\r\n\r\nrandom_forest_workflow\r\n\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Recipe\r\nModel: rand_forest()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n1 Recipe Step\r\n\r\n* step_dummy()\r\n\r\n-- Model -------------------------------------------------------------\r\nRandom Forest Model Specification (regression)\r\n\r\nMain Arguments:\r\n  mtry = tune()\r\n  trees = tune()\r\n\r\nComputational engine: ranger \r\n\r\nFinding the best values using a grid. We will also fold using cross validation\r\n\r\n\r\nset.seed(1000)\r\n\r\nprice_folds <- vfold_cv(london_train)\r\n\r\nrandom_forest_grid <- tune_grid(\r\n  random_forest_workflow,\r\n  resamples = price_folds,\r\n  grid = 20\r\n)\r\n\r\nrandom_forest_grid\r\n\r\n\r\n# Tuning results\r\n# 10-fold cross-validation \r\n# A tibble: 10 x 4\r\n   splits              id     .metrics          .notes          \r\n   <list>              <chr>  <list>            <list>          \r\n 1 <rsplit [2349/261]> Fold01 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 2 <rsplit [2349/261]> Fold02 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 3 <rsplit [2349/261]> Fold03 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 4 <rsplit [2349/261]> Fold04 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 5 <rsplit [2349/261]> Fold05 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 6 <rsplit [2349/261]> Fold06 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 7 <rsplit [2349/261]> Fold07 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 8 <rsplit [2349/261]> Fold08 <tibble [40 x 6]> <tibble [0 x 1]>\r\n 9 <rsplit [2349/261]> Fold09 <tibble [40 x 6]> <tibble [0 x 1]>\r\n10 <rsplit [2349/261]> Fold10 <tibble [40 x 6]> <tibble [0 x 1]>\r\n\r\n\r\n\r\nrandom_forest_grid %>%\r\n  collect_metrics()\r\n\r\n\r\n# A tibble: 40 x 8\r\n    mtry trees .metric .estimator  mean     n std_err .config\r\n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \r\n 1     4  1204 rmse    standard   0.163    10 0.00273 Model01\r\n 2     4  1204 rsq     standard   0.754    10 0.00799 Model01\r\n 3     2  1683 rmse    standard   0.161    10 0.00288 Model02\r\n 4     2  1683 rsq     standard   0.767    10 0.00596 Model02\r\n 5     6   806 rmse    standard   0.172    10 0.00284 Model03\r\n 6     6   806 rsq     standard   0.729    10 0.00885 Model03\r\n 7     2   191 rmse    standard   0.161    10 0.00292 Model04\r\n 8     2   191 rsq     standard   0.767    10 0.00610 Model04\r\n 9     5  1733 rmse    standard   0.169    10 0.00282 Model05\r\n10     5  1733 rsq     standard   0.739    10 0.00862 Model05\r\n# ... with 30 more rows\r\n\r\nBased on the graph below, we could see that most of the hyperparameters produced good results (low RMSE). It can also be seen that mtry = 1 produced the worst RMSE.\r\n\r\n\r\nrandom_forest_grid %>%\r\n  collect_metrics() %>%\r\n  filter(.metric == \"rmse\") %>%\r\n  mutate(mtry = as.factor(mtry)) %>%\r\n  ggplot(aes(x = trees, y = mean, colour = mtry)) +\r\n  geom_point() +\r\n  labs(y = \"RMSE\")\r\n\r\n\r\n\r\n\r\nSelect the best model\r\n\r\n\r\nbest_rmse <- select_best(random_forest_grid, \"rmse\")\r\n\r\nfinal_random_forest <- finalize_model(\r\n  random_forest_model,\r\n  best_rmse\r\n)\r\n\r\nfinal_random_forest\r\n\r\n\r\nRandom Forest Model Specification (regression)\r\n\r\nMain Arguments:\r\n  mtry = 3\r\n  trees = 1185\r\n\r\nComputational engine: ranger \r\n\r\nFit the best model\r\nBased on the RMSE and R-square, I would say that is quite decent.\r\n\r\n\r\nfinal_workflow <- workflow() %>%\r\n  add_recipe(price_recipe) %>%\r\n  add_model(final_random_forest)\r\n\r\nfinal_model<- final_workflow %>%\r\n  last_fit(london_split)\r\n\r\nfinal_model %>%\r\n  collect_metrics()\r\n\r\n\r\n# A tibble: 2 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard       0.155\r\n2 rsq     standard       0.786\r\n\r\nSave prediction results\r\n\r\n\r\npredict_result <- final_model %>%\r\n  collect_predictions() %>%\r\n  select(.pred) %>%\r\n  bind_cols(london_test) %>%\r\n  rename(predicted_random_forest = .pred)\r\n\r\n\r\n\r\nSummarise the results\r\n\r\n\r\npredict_result %>%\r\n  ggplot(aes(x = price_log, y = predicted_random_forest, \r\n             colour = house_type))+\r\n  geom_point()+\r\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5)+\r\n  facet_wrap(house_type ~.) +\r\n  labs(x = \"Price (Log 10)\",\r\n       y = \"Predicted Log Price\") +\r\n  theme(legend.position = \"none\") \r\n\r\n\r\n\r\n\r\nFrom this graph, we could get an overview of the actual price vs the predicted price. Please let me know your thoughts on this.\r\nThanks\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-18-houseprice/houseprice_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-18T16:04:25+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-18-fuzzymatching/",
    "title": "STORE MATCHING USING FUZZYJOIN IN R",
    "description": "Matching stores between two datasets.",
    "author": [
      {
        "name": "Zahier Nasrudin",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\r\nIntroduction\r\nLast week, we had a request from a client. We were given a list of 600 stores and I needed to match those stores with the stores that we have in our database.\r\nSo, I used “fuzzyjoin” package in R to find the similarity between these stores based on the address list given.\r\nFor this project, I am not going to use the dataset given by the client (confidential). Instead, I am going to use a dataset available in Kaggle.\r\nDataset\r\nIt is a list of Target stores (as of April 2017). It has a lot of useful attributes such as operating hour, open date, and latitude & longitude. However, I will only be requiring the address column for this project:\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(janitor)\r\nlibrary(readr)\r\nlibrary(fuzzyjoin)\r\nlibrary(here)\r\n \r\ntargets <- read_csv(here(\"_posts/2021-03-18-fuzzymatching/1. Input Files/targets.csv\")) %>%\r\n  clean_names() %>%\r\n  select(id, address_formatted_address)\r\n\r\n\r\n\r\nI will copy this data set, but this time I will remove the vowels, and the id column\r\n\r\n\r\ntargets_missing <- targets %>%\r\n  select(-id) %>%\r\n  mutate(address_formatted_address = str_remove_all(\r\n    tolower(address_formatted_address), \r\n    \"[aeiou]\"))\r\n\r\n\r\n\r\nNext, I will use stringdist_left_join() to find the stores between the targets data frame and “targets_missing” data frame. I will then use slice_min to get the minimum value (distance score) for each of the store because I just want only a match per store, and I want it to be the closest distance. (Hence, the minimum value per store).\r\n\r\n\r\nstores_matching <- targets %>%\r\n  stringdist_left_join(targets_missing, by = \"address_formatted_address\",\r\n                       method = \"cosine\", distance_col = \"score\")\r\n \r\nstores_matching <- stores_matching %>%\r\n  group_by(address_formatted_address.x) %>%\r\n  slice_min(order_by = score)\r\n\r\n\r\n\r\n\r\n\r\nstores_matching %>%\r\n  head() %>%\r\n  kableExtra::kbl() %>%\r\n  kableExtra::kable_styling()\r\n\r\n\r\n\r\nid\r\n\r\n\r\naddress_formatted_address.x\r\n\r\n\r\naddress_formatted_address.y\r\n\r\n\r\nscore\r\n\r\n\r\n2258\r\n\r\n\r\n1 Hawes Way, Stoughton, MA 02072-1162\r\n\r\n\r\n1 hws wy, stghtn, m 02072-1162\r\n\r\n\r\n0.1237150\r\n\r\n\r\n1443\r\n\r\n\r\n1 Mifflin St, Philadelphia, PA 19148-2017\r\n\r\n\r\n1 mffln st, phldlph, p 19148-2017\r\n\r\n\r\n0.1808536\r\n\r\n\r\n1229\r\n\r\n\r\n1 Mystic View Rd, Everett, MA 02149-2428\r\n\r\n\r\n1 mystc vw rd, vrtt, m 02149-2428\r\n\r\n\r\n0.1444322\r\n\r\n\r\n1296\r\n\r\n\r\n1 N Galleria Dr, Middletown, NY 10941-3029\r\n\r\n\r\n1 n gllr dr, mddltwn, ny 10941-3029\r\n\r\n\r\n0.1454589\r\n\r\n\r\n2799\r\n\r\n\r\n1 S State St, Chicago, IL 60603\r\n\r\n\r\n1 s stt st, chcg, l 60603\r\n\r\n\r\n0.1917096\r\n\r\n\r\n1492\r\n\r\n\r\n1 Sangertown Sq Ste 3, New Hartford, NY 13413-1595\r\n\r\n\r\n1 sngrtwn sq st 3, nw hrtfrd, ny 13413-1595\r\n\r\n\r\n0.1488589\r\n\r\n\r\nClosing\r\nNow, this method definitely helped me when I had to find at least 600 stores. And no, I definitely did not consider to do it manually.\r\nPlease let me know what you think of this and what you could have done differently. Thanks for reading and have a nice day\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-18-fuzzymatching/images/market.jpg",
    "last_modified": "2021-03-18T16:40:05+08:00",
    "input_file": {}
  }
]
